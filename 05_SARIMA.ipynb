{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88beb4b8-1496-46fe-a645-aa462c8e62d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fded4a61-8f77-42cf-b905-16cc00116a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "STATION = 235\n",
    "FREQ = '4w'\n",
    "PERIOD = 13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45dbac4e-c95c-44b7-b94d-424dab4e53ea",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Functions and helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e896e2f7-48ec-45f3-8d4b-59fad95c8efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil.parser import parse \n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from statsmodels.tsa.stattools import adfuller, kpss\n",
    "from arch.unitroot import PhillipsPerron\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "plt.rcParams.update({'figure.figsize': (10, 7), 'figure.dpi': 120})\n",
    "\n",
    "def get_specific_station_data(station, freq, filter_year = 1951):\n",
    "    hadoopUrl = 'hdfs://hadoop-vm.internal.cloudapp.net:9000'\n",
    "    data_files = f'{hadoopUrl}/precipitation/data/{station}/*.parquet'\n",
    "\n",
    "    # Obtain dataset\n",
    "    df = spark.read.parquet(data_files) \\\n",
    "            .withColumn(\"precipitation\", col(\"precipitation\").cast(\"float\")) \\\n",
    "            .select(\"date\",\"precipitation\") \\\n",
    "            .toPandas()\n",
    "    \n",
    "    # Set the date column as the index and ensure it's a DatetimeIndex\n",
    "    df.set_index('date', inplace=True)\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    \n",
    "    # Filter years\n",
    "    df = df[df.index.year > filter_year] # Only 1951 onwards\n",
    "    \n",
    "    # Set the frequency data set\n",
    "    df = df.resample(freq).mean()\n",
    "    \n",
    "    # drop null\n",
    "    df.reset_index(inplace=True)\n",
    "    df = df.dropna(axis=0)\n",
    "    \n",
    "    # set index\n",
    "    df.set_index('date', inplace=True)\n",
    "    df = df.sort_index()\n",
    "    return df\n",
    "\n",
    "\n",
    "def test_stationarity(time_series):\n",
    "    # Perform ADF test\n",
    "    adf_result = adfuller(time_series, autolag='AIC')\n",
    "    print(\"ADF Test:\")\n",
    "    print(\"=========\")\n",
    "    print(f\"Test statistic: {adf_result[0]}\")\n",
    "    print(f\"null_hypothesis: the time series is non-stationary\")\n",
    "    p_value = adf_result[1]\n",
    "    print(f\"P-value: {p_value}\")\n",
    "    print(\"Critical values:\")\n",
    "    for key, value in adf_result[4].items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    \n",
    "    result = \"STATIONARY\" if p_value < 0.05 else \"NON-STATIONARY\"\n",
    "    print(result)\n",
    "    \n",
    "\n",
    "    print(\"\\nKPSS Test:\")\n",
    "    print(\"============\")\n",
    "    kpss_result = kpss(time_series, regression='ct')  # 'ct' for constant and trend\n",
    "    print(f\"null_hypothesis: the time series is trend stationary\")\n",
    "    print(f\"Test statistic: {kpss_result[0]}\")\n",
    "    print(f\"P-value: {kpss_result[1]}\")\n",
    "    p_value = kpss_result[1]\n",
    "    print(\"Critical values:\")\n",
    "    for key, value in kpss_result[3].items():\n",
    "        print(f\"{key}: {value}\")\n",
    "        \n",
    "    result = \"NON-STATIONARY\" if p_value < 0.05 else \"STATIONARY\"\n",
    "    print(result)\n",
    "\n",
    "\n",
    "def remove_outlier(df):\n",
    "    # Calculate the IQR\n",
    "    Q1 = df['precipitation'].quantile(0.25)\n",
    "    Q3 = df['precipitation'].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    # Define the bounds for outliers\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    # Filter out the outliers\n",
    "    output = df[(df['precipitation'] >= lower_bound) & (df['precipitation'] <= upper_bound)]\n",
    "    print(f\"Records removed: {df.shape[0] - output.shape[0]}\")\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368176c9-6108-4188-a366-c5f5b4bc881e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# SARIMA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6026359-a8cc-49bd-bca1-19b6ab9ba18b",
   "metadata": {},
   "source": [
    "## Stationary test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25408961-3e88-4b34-a293-6d60d23e8ee0",
   "metadata": {},
   "source": [
    "Refer to Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb85b78-4b80-4d96-bf1b-129deb4dda56",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba82336-7cd5-45aa-bfa0-7c90c2baa45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "df_seasson = get_specific_station_data(STATION,FREQ,1800)\n",
    "\n",
    "scaler = MinMaxScaler() # So it can be compared with LSTM\n",
    "df_seasson = scaler.fit_transform(df_seasson)\n",
    "\n",
    "train_data, test_data = train_test_split(df_seasson, test_size=0.2,shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4843f73-9116-4a48-a53f-318342793b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "plot_acf(train_data, lags=20, ax=ax1)\n",
    "plot_pacf(train_data, lags=20, ax=ax2)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b6da80-f8f8-45d8-837e-34480256f18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 1\n",
    "d = 0\n",
    "q = 1\n",
    "s = PERIOD  # Seasonal period (e.g., 12 months)\n",
    "P = 1\n",
    "D = 1\n",
    "Q = 1\n",
    "\n",
    "model = SARIMAX(train_data, order=(p, d, q), seasonal_order=(P, D, Q, s))\n",
    "results = model.fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1ba838-261c-4e5f-9ebf-b862a50dc6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = results.get_forecast(steps=len(test_data))\n",
    "mean_predictions = predictions.predicted_mean\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "test_rmse = np.sqrt(mean_squared_error(test_data, mean_predictions))\n",
    "test_mae = mean_absolute_error(test_data, mean_predictions)\n",
    "#test_mape = np.mean(np.abs((test_data - mean_predictions) / test_data)) * 100\n",
    "#test_mase = test_mae / (np.mean(np.abs(test_data[1:] - test_data[:-1])))\n",
    "\n",
    "print(f\"Test RMSE: {test_rmse:.2f}\")\n",
    "print(f\"Test MAE: {test_mae:.2f}\")\n",
    "#print(f\"Test MAPE: {test_mape:.2f}%\")\n",
    "#print(f\"Test MASE: {test_mase:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d48fad1-cbd6-4836-9190-fc5d3cf48967",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "#plt.plot(train_data, label='Training Data')\n",
    "plt.plot(test_data, label='Actual values')\n",
    "plt.plot(mean_predictions, label='Predictions', linestyle='dashed')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Precipitation')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8acecf-07c9-4eda-b484-1721d87d9af5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## AUTO ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0230be5c-6f31-4b69-b990-5844f69d80a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install pmdarima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680c989e-aed4-4bca-94d0-3abffbd14a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pmdarima as pm\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "df_seasson = get_specific_station_data(STATION,FREQ,1800)\n",
    "df_seasson = scaler.fit_transform(df_seasson)\n",
    "train_data, test_data = train_test_split(df_seasson, test_size=0.2,shuffle=False)\n",
    "\n",
    "auto_model = pm.auto_arima(train_data,\n",
    "                      seasonal=True,\n",
    "                      m=PERIOD,  # Seasonal frequency\n",
    "                      start_p=0, start_q=0, max_p=3, max_q=3,  # Non-seasonal parameters\n",
    "                      start_P=0, start_Q=0, max_P=3, max_Q=3,  # Seasonal parameters\n",
    "                      d=0, D=3,  # Orders of differencing\n",
    "                      trace=False,  # Print search information\n",
    "                      error_action='ignore',\n",
    "                      suppress_warnings=True,\n",
    "                      stepwise=True)  # Stepwise search for efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e05651-35df-4a16-bce9-ad8be641f42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best SARIMA Model: {auto_model.order}, {auto_model.seasonal_order}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244d963f-bde2-45fb-8f7d-e1288cdf3e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SARIMAX(train_data, order=auto_model.order, seasonal_order=auto_model.seasonal_order)\n",
    "results = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dbb7e0-f45e-4844-a1f2-1f1d801502f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = results.get_forecast(steps=len(test_data))\n",
    "mean_predictions = predictions.predicted_mean\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(test_data, mean_predictions))\n",
    "print(f\"RMSE: {rmse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24de9f67-efab-405b-a136-7a01f453536f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "#plt.plot(train_data, label='Training Data')\n",
    "plt.plot(test_data, label='Actual values')\n",
    "plt.plot(mean_predictions, label='Predictions', linestyle='dashed')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Precipitation')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143e61c1-b741-47a8-9f30-4b634097855f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 - Spark (local)",
   "language": "python",
   "name": "spark-3-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
