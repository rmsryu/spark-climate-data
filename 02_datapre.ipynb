{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ed7553f-425c-4504-b420-86eb996f623a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://rmsryu-vm.internal.cloudapp.net:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7d0dd1",
   "metadata": {},
   "source": [
    "# Loading (remote) E-OBS observations and summer days (SU) calculation\n",
    "\n",
    "Git repo: https://github.com/rmsryu/spark-climate-data.git\n",
    "Data set: https://www.ecad.eu/download/ensembles/download.php\n",
    "\n",
    "E-OBS data is loaded from the remote OpenDAP repository at KNMI. Please, note that when new versions become available the link is no longer valid since the URL is updated with the version number. Therefore, if the code below doesn't work please check the current E-OBS version and update the URL (https://www.ecad.eu/download/ensembles/download.php). \n",
    "\n",
    "# Study of daily precipitation from Station Aalsmeer Netherlands\n",
    "Data source: https://climexp.knmi.nl/data/rrrr458.dat\n",
    "Data provide by: Royal Netherlands Meteorological Institute (KNMI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77387014",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark import SparkFiles\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c9dcb4e-d35d-48cc-b82a-59011025d8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80e6ce84-7181-4764-a5f0-b1a12076e030",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "hadoopUrl = 'hdfs://hadoop-vm.internal.cloudapp.net:9000'\n",
    "data_files = f'{hadoopUrl}/precipitation/data/*/*.parquet'\n",
    "# Obtain dataset\n",
    "data = spark.read.parquet(data_files)\n",
    "data = data.withColumn(\"precipitation\", col(\"precipitation\").cast(\"float\"))\n",
    "data = data.withColumn(\"date\", col(\"date\").cast(\"date\"))\n",
    "data.createOrReplaceTempView(\"precipitations\")\n",
    "data.cache()\n",
    "\n",
    "# Read Station Info\n",
    "schema_stations = StructType([\n",
    "        StructField(\"station\", StringType(), True),\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"latitude\", FloatType(), True),\n",
    "        StructField(\"longitude\", FloatType(), True),\n",
    "        StructField(\"website\", StringType(), True),\n",
    "        StructField(\"rawdata\", StringType(), True),\n",
    "        StructField(\"filename\", StringType(), True),\n",
    "        StructField(\"from\", StringType(), True),\n",
    "        StructField(\"to\", StringType(), True)\n",
    "    ])\n",
    "station_files = 'hdfs://hadoop-vm.internal.cloudapp.net:9000/precipitation/data/stations/*.csv'\n",
    "stations = spark.read.csv(station_files,schema=schema_stations)\n",
    "stations.withColumn(\"from\", col(\"from\").cast(\"date\"))\n",
    "stations.withColumn(\"to\", col(\"to\").cast(\"date\"))\n",
    "stations.createOrReplaceTempView(\"stations\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eaf9211c-2afb-4492-9756-1b14831f27c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+\n",
      "|summary|           station|    precipitation|\n",
      "+-------+------------------+-----------------+\n",
      "|  count|          13120148|         13120148|\n",
      "|   mean|483.02698064076714| 2.13989743152896|\n",
      "| stddev|287.72368836944077|4.339730993138887|\n",
      "|    min|               001|              0.0|\n",
      "|    25%|             228.0|              0.0|\n",
      "|    50%|             466.0|              0.1|\n",
      "|    75%|             737.0|              2.4|\n",
      "|    max|               983|            208.0|\n",
      "+-------+------------------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data.summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d58b7956-6982-4bd0-8a1f-1a7a647cae19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------+---------+--------------------+--------------------+------------+----------+----------+\n",
      "|station|                name|latitude|longitude|             website|             rawdata|    filename|      from|        to|\n",
      "+-------+--------------------+--------+---------+--------------------+--------------------+------------+----------+----------+\n",
      "|    458|            Aalsmeer|   52.25|     4.77|https://climexp.k...|https://climexp.k...|rrrr458.data|1927-01-02|2023-03-10|\n",
      "|    040|   Aalsum bij Dokkum|  -999.9|   -999.9|https://climexp.k...|https://climexp.k...|rrrr040.data|1892-12-29|1925-09-30|\n",
      "|    680|              Aalten|    51.9|     6.55|https://climexp.k...|https://climexp.k...|rrrr680.data|1904-08-02|2023-03-10|\n",
      "|    711|          Aardenburg|  -999.9|   -999.9|https://climexp.k...|https://climexp.k...|rrrr711.data|1873-12-01|1887-06-30|\n",
      "|    572|             Abcoude|   52.25|     4.97|https://climexp.k...|https://climexp.k...|rrrr572.data|1930-09-02|2023-03-10|\n",
      "|    045|              Akkrum|  -999.9|   -999.9|https://climexp.k...|https://climexp.k...|rrrr045.data|1882-10-01|1937-04-30|\n",
      "|    089|              Akkrum|   53.05|     5.82|https://climexp.k...|https://climexp.k...|rrrr089.data|1995-10-01|2023-03-10|\n",
      "|    200|             Alkmaar|  -999.9|   -999.9|https://climexp.k...|https://climexp.k...|rrrr200.data|1880-03-02|1941-09-01|\n",
      "|    664|              Almelo|   52.33|     6.67|https://climexp.k...|https://climexp.k...|rrrr664.data|1880-08-02|2023-03-10|\n",
      "|    678|               Almen|   52.15|     6.32|https://climexp.k...|https://climexp.k...|rrrr678.data|1950-03-02|2023-03-10|\n",
      "|    363|Alteveer (Hoogeveen)|  -999.9|   -999.9|https://climexp.k...|https://climexp.k...|rrrr363.data|1916-11-01|1922-09-30|\n",
      "|    169|                Amen|  -999.9|   -999.9|https://climexp.k...|https://climexp.k...|rrrr169.data|1968-01-01|1975-01-01|\n",
      "|    560|           Amerongen|   51.98|     5.45|https://climexp.k...|https://climexp.k...|rrrr560.data|1910-04-09|2023-03-10|\n",
      "|    515|          Amersfoort|  -999.9|   -999.9|https://climexp.k...|https://climexp.k...|rrrr515.data|1880-05-01|1948-04-01|\n",
      "|    529|Amersfoort (Tutei...|  -999.9|   -999.9|https://climexp.k...|https://climexp.k...|rrrr529.data|1928-01-01|1934-12-31|\n",
      "|    910|          Ammerzoden|   51.73|      5.2|https://climexp.k...|https://climexp.k...|rrrr910.data|1932-10-14|2023-03-10|\n",
      "|    441|           Amsterdam|   52.37|     4.92|https://climexp.k...|https://climexp.k...|rrrr441.data|1916-01-01|2015-01-01|\n",
      "|    433|Amsterdam Anwi (Fil)|  -999.9|   -999.9|https://climexp.k...|https://climexp.k...|rrrr433.data|1880-01-01|1962-01-01|\n",
      "|    835|               Andel|   51.78|     5.05|https://climexp.k...|https://climexp.k...|rrrr835.data|1912-02-24|2023-03-10|\n",
      "|    205|              Andijk|  -999.9|   -999.9|https://climexp.k...|https://climexp.k...|rrrr205.data|1904-01-01|1935-10-31|\n",
      "+-------+--------------------+--------+---------+--------------------+--------------------+------------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stations.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d56bd7-b011-43dd-99ad-552edcd8e7f5",
   "metadata": {},
   "source": [
    "# Stations stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2d78c67-904e-4d66-8b77-ecaaeb154313",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+---------+-----+------------------+------------------+---+-----+-------------------+------------------+-----+\n",
      "|station|min_year|num_years|count|              mean|            stddev|min|25pct|              50pct|             75pct|  max|\n",
      "+-------+--------+---------+-----+------------------+------------------+---+-----+-------------------+------------------+-----+\n",
      "|    001|    1940|       83|29556| 2.174519556030149| 4.370148656161713|0.0|  0.0|0.10000000149011612|               2.5| 76.6|\n",
      "|    003|    1902|       51|18482|2.0838329184571815| 4.277456332945257|0.0|  0.0|0.10000000149011612| 2.299999952316284| 78.4|\n",
      "|    004|    1904|        1|  419| 1.617899758558996|3.4800441747162214|0.0|  0.0|0.10000000149011612| 1.399999976158142| 26.9|\n",
      "|    006|    1905|       38|13970|1.7968360776213679| 3.740703703355201|0.0|  0.0|0.10000000149011612| 1.899999976158142| 82.1|\n",
      "|    007|    1906|       44|15101|1.9131845566100272|3.8756137261021313|0.0|  0.0|                0.0| 2.200000047683716| 64.7|\n",
      "|    008|    1937|        3| 1096|1.5393248179614762|3.4753844346339866|0.0|  0.0|                0.0| 1.600000023841858| 38.0|\n",
      "|    009|    1851|      121|44406|1.8896342841528726|3.8899340549232315|0.0|  0.0|0.10000000149011612|2.0999999046325684| 83.3|\n",
      "|    010|    1872|      151|52324|2.0548104115429116| 4.179136243163778|0.0|  0.0|                0.0|2.4000000953674316|109.4|\n",
      "|    011|    1876|      147|53758| 2.082222180832206| 4.201727303440663|0.0|  0.0|0.10000000149011612|2.4000000953674316| 91.0|\n",
      "|    012|    1879|      144|42144| 2.113351841111213| 4.162499025008069|0.0|  0.0|                0.0|               2.5| 78.9|\n",
      "|    013|    1885|      100|15262| 2.010267331447858|3.9819975326866857|0.0|  0.0|0.20000000298023224| 2.299999952316284| 57.9|\n",
      "|    014|    1885|       67| 7476|1.7431514188726998| 3.773894477530973|0.0|  0.0|                0.0|1.7999999523162842| 56.0|\n",
      "|    015|    1885|      138|49809| 2.089361360136142| 4.329440914202082|0.0|  0.0|                0.0|2.4000000953674316| 85.5|\n",
      "|    016|    1891|      132|48159|2.0030503136297924| 4.152152517357461|0.0|  0.0|0.10000000149011612| 2.200000047683716|112.3|\n",
      "|    017|    1894|      129|29843|2.1887645339229724| 4.318287714354467|0.0|  0.0|0.20000000298023224|               2.5| 57.5|\n",
      "|    018|    1916|      107|36771|2.2269125135079992| 4.474478831477688|0.0|  0.0|0.10000000149011612| 2.700000047683716| 87.2|\n",
      "|    019|    1929|       94|34221|2.1881330180013485|4.3172841497787715|0.0|  0.0|0.20000000298023224|               2.5| 67.5|\n",
      "|    021|    1940|       83|29594|2.1730249374286683|4.3682149079767045|0.0|  0.0|0.10000000149011612|               2.5| 76.6|\n",
      "|    022|    1950|       73|26537|2.2094773349523833| 4.390459839096478|0.0|  0.0|0.20000000298023224|               2.5| 68.3|\n",
      "|    023|    1876|        9| 1824|1.8230263121066648| 3.806213185611394|0.0|  0.0|                0.0|               2.0| 38.2|\n",
      "+-------+--------+---------+-----+------------------+------------------+---+-----+-------------------+------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT station,\\\n",
    "              year(min(date)) min_year, \\\n",
    "              year(max(date)) - year(min(date)) num_years, \\\n",
    "              count(precipitation) count,\\\n",
    "              mean(precipitation) mean, \\\n",
    "              stddev(precipitation) stddev, \\\n",
    "              min(precipitation) min, \\\n",
    "              percentile(precipitation,0.25) 25pct, \\\n",
    "              percentile(precipitation,0.50) 50pct, \\\n",
    "              percentile(precipitation,0.75) 75pct, \\\n",
    "              max(precipitation) max \\\n",
    "            FROM precipitations GROUP BY station ORDER BY station\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52111369-e620-4ae8-9614-380c1ac21c9f",
   "metadata": {},
   "source": [
    "## Reliabality\n",
    "\n",
    "To create a relevant and reliable dataset for average precipitation the following will be considered:\n",
    "\n",
    "**Data selection**: Focus on the period with the most comprehensive and consistent data choosing a time frame when a higher number of weather stations are available.\n",
    "\n",
    "**Handling missing data**: For stations with missing data or long gaps consider remove or imputate missing values.\n",
    "\n",
    "**Temporal trends**: Analyze the dataset for any systematic temporal trends (e.g., increasing or decreasing precipitation over the years). \n",
    "\n",
    "**Weighting**: Contribution of each station based on factors like the length of the available data, the quality of the measurements, or the spatial coverage. Spatial coverage per station is 10Km\n",
    "\n",
    "**Spatial coverage**: Stations spread homogenusly on the Netherland area. Some of stations do not include spatial information. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "31047580-4c33-4fed-b34d-bd5f79e950e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from operator import add\n",
    "\n",
    "def find_desired_year_stations(minimum_year = 1978, min_desired_data_points_per_year = 360):    \n",
    "    # Get stations grouped by year and calculate desired number of records per year\n",
    "    stations_years_complete  = spark.sql(f\"SELECT station, year(date) year, count(precipitation) pre_year FROM precipitations where year(date) >= {minimum_year} GROUP BY station, year(date)\")\n",
    "    stations_years_complete = stations_years_complete.filter(col(\"pre_year\") > min_desired_data_points_per_year)\n",
    "    stations_years_complete = stations_years_complete.select(\"station\",\"year\").orderBy(\"station\",\"year\")\n",
    "\n",
    "    # Group by 'station' and pivot on 'year'\n",
    "    df_transposed = stations_years_complete.groupBy(\"station\").pivot(\"year\").avg(\"year\")\n",
    "\n",
    "    # Count the number of non null years per station\n",
    "    year_columns = df_transposed.columns[1:]\n",
    "    # expresion to filter year column not null\n",
    "    count_years_expr = reduce(add, [when(col(year_col).isNotNull(), 1).otherwise(0) for year_col in year_columns])\n",
    "    df_transposed = df_transposed.withColumn(\"total_years\", count_years_expr)\n",
    "\n",
    "    # print(f\"Looking for stations with years {len(year_columns)}\")\n",
    "    stations_for_analysis = df_transposed.filter(col(\"total_years\") == len(year_columns)).select(\"station\",\"total_years\")\n",
    "    stations_filter = [item.station for item in stations_for_analysis.collect()]\n",
    "    # print(f\"A total of {len(stations_filter)} stations to be included\")\n",
    "    \n",
    "    return stations_filter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8e43e849-7a73-4078-bce3-54f3661afdff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "starting_years = np.arange(1950, 1960)\n",
    "# Iterate to find best year\n",
    "year_list = []\n",
    "result = []\n",
    "best_num_of_stations = 0\n",
    "for year in starting_years:\n",
    "    temp = len(find_desired_year_stations(year, 360))\n",
    "    result.append((year,temp))\n",
    "    best_initial_year =  year if (temp > best_num_of_stations) else best_initial_year\n",
    "    best_num_of_stations =  temp if (temp > best_num_of_stations) else best_num_of_stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ca0cf2ec-39e6-41c8-8b3a-79882d404d7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1950, 132),\n",
       " (1951, 144),\n",
       " (1952, 150),\n",
       " (1953, 156),\n",
       " (1954, 174),\n",
       " (1955, 179),\n",
       " (1956, 188),\n",
       " (1957, 204),\n",
       " (1958, 212),\n",
       " (1959, 218)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ee367b45-9af3-448b-a56d-591dc0d81061",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Selecting year 1955 as baseline with 179 station complete stations datasets and minimum 360 observations\n",
    "stations_filter = find_desired_year_stations(1955, 360)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "adc61588-8854-4462-ad87-66052d2bd343",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stations_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "eb05d180-f40a-45f2-b3cd-e513cbcf67d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 957:=====================>                                  (8 + 4) / 21]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------------------+\n",
      "|summary|      precipitation|                  w|\n",
      "+-------+-------------------+-------------------+\n",
      "|  count|              24541|              24541|\n",
      "|   mean| 2.2576964836099185| 178.99690314168126|\n",
      "| stddev|  3.513736968249654|0.07864081706020871|\n",
      "|    min|                0.0|                177|\n",
      "|    25%|0.03240223528619585|                179|\n",
      "|    50%| 0.6435754158476877|                179|\n",
      "|    75%|  3.143575413183793|                179|\n",
      "|    max|  45.87486042257128|                179|\n",
      "+-------+-------------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data = data.filter(data.station.isin(stations_filter))\n",
    "data.createOrReplaceTempView(\"final_ds\")\n",
    "df_final = spark.sql(f\"SELECT date, avg(precipitation) precipitation, count(precipitation) w from final_ds where year(date) > {candiate_min_year} group by date\")\n",
    "df_final.summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "50f7be58-7605-43b2-b912-075d44046ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Save to hadoop\n",
    "df_final.write.mode(\"overwrite\").parquet(f\"{hadoopUrl}/precipitation/data/agg/from/{candiate_min_year}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e534788-1dfe-4d03-9f40-51cfb5c009e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 - Spark (local)",
   "language": "python",
   "name": "spark-3-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
