{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f29a32d5-d460-4369-ac02-41bf240cc73d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://rmsryu-vm.internal.cloudapp.net:4046\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0c187bb-fcdc-45ab-b589-4be488bdb69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install elephas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3cd9bb2-78e4-4319-b54e-e5d9b8d5f1c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-07 19:54:35.871957: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-07 19:54:35.987111: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/intel/compilers_and_libraries_2018.1.163/linux/tbb/lib/intel64_lin/gcc4.7:/opt/intel/compilers_and_libraries_2018.1.163/linux/compiler/lib/intel64_lin:/opt/intel/compilers_and_libraries_2018.1.163/linux/mkl/lib/intel64_lin::/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64/\n",
      "2023-04-07 19:54:35.987130: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-04-07 19:54:36.018645: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-04-07 19:54:36.578760: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/intel/compilers_and_libraries_2018.1.163/linux/tbb/lib/intel64_lin/gcc4.7:/opt/intel/compilers_and_libraries_2018.1.163/linux/compiler/lib/intel64_lin:/opt/intel/compilers_and_libraries_2018.1.163/linux/mkl/lib/intel64_lin::/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64/\n",
      "2023-04-07 19:54:36.578865: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/intel/compilers_and_libraries_2018.1.163/linux/tbb/lib/intel64_lin/gcc4.7:/opt/intel/compilers_and_libraries_2018.1.163/linux/compiler/lib/intel64_lin:/opt/intel/compilers_and_libraries_2018.1.163/linux/mkl/lib/intel64_lin::/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64/\n",
      "2023-04-07 19:54:36.578873: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, GRU\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql.functions import *\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "def get_specific_station_data(station, freq, filter_year = 1951):\n",
    "    hadoopUrl = 'hdfs://hadoop-vm.internal.cloudapp.net:9000'\n",
    "    data_files = f'{hadoopUrl}/precipitation/data/{station}/*.parquet'\n",
    "\n",
    "    # Obtain dataset\n",
    "    df = spark.read.parquet(data_files) \\\n",
    "            .withColumn(\"precipitation\", col(\"precipitation\").cast(\"float\")) \\\n",
    "            .select(\"date\",\"precipitation\") \\\n",
    "            .toPandas()\n",
    "    \n",
    "    # Set the date column as the index and ensure it's a DatetimeIndex\n",
    "    df.set_index('date', inplace=True)\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    \n",
    "    # Filter years\n",
    "    df = df[df.index.year > filter_year] # Only 1951 onwards\n",
    "    \n",
    "    # Set the frequency data set\n",
    "    df = df.resample(freq).mean()\n",
    "    \n",
    "    # drop null\n",
    "    df.reset_index(inplace=True)\n",
    "    df = df.dropna(axis=0)\n",
    "    \n",
    "    # set index\n",
    "    df.set_index('date', inplace=True)\n",
    "    df = df.sort_index()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdc0066-74ce-46f8-aa5f-4ed6192fc9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "28/28 - 4s - loss: 0.0304 - val_loss: 0.0169 - 4s/epoch - 158ms/step\n",
      "Epoch 2/20\n",
      "28/28 - 0s - loss: 0.0202 - val_loss: 0.0164 - 416ms/epoch - 15ms/step\n",
      "Epoch 3/20\n",
      "28/28 - 0s - loss: 0.0199 - val_loss: 0.0163 - 481ms/epoch - 17ms/step\n",
      "Epoch 4/20\n",
      "28/28 - 0s - loss: 0.0197 - val_loss: 0.0161 - 437ms/epoch - 16ms/step\n",
      "Epoch 5/20\n",
      "28/28 - 0s - loss: 0.0197 - val_loss: 0.0161 - 439ms/epoch - 16ms/step\n",
      "Epoch 6/20\n",
      "28/28 - 0s - loss: 0.0197 - val_loss: 0.0161 - 437ms/epoch - 16ms/step\n",
      "Epoch 7/20\n",
      "28/28 - 0s - loss: 0.0194 - val_loss: 0.0159 - 382ms/epoch - 14ms/step\n",
      "Epoch 8/20\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess the data\n",
    "station = 235\n",
    "data = get_specific_station_data(station,'4w',1800)\n",
    "\n",
    "# Scale the precipitation data\n",
    "scaler = MinMaxScaler()\n",
    "data[\"precipitation\"] = scaler.fit_transform(data[[\"precipitation\"]])\n",
    "\n",
    "# Prepare the input and output for the RNN model\n",
    "def prepare_data(data, window_size):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - window_size):\n",
    "        X.append(data[i : (i + window_size)])\n",
    "        y.append(data[i + window_size])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "window_size = 10\n",
    "X, y = prepare_data(data[\"precipitation\"].values, window_size)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reshape the input to be 3D (samples, timesteps, features)\n",
    "X_train = X_train.reshape(X_train.shape[0], window_size, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], window_size, 1)\n",
    "\n",
    "# Create the RNN model with GRU units\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(GRU(units=50, activation=\"relu\", input_shape=(window_size, 1), return_sequences=True))\n",
    "    #model.add(Dropout(0.2))\n",
    "    model.add(GRU(units=50, activation=\"relu\", return_sequences=True))\n",
    "    #model.add(Dropout(0.2))\n",
    "    model.add(GRU(units=50, activation=\"relu\"))\n",
    "    #model.add(Dropout(0.2))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n",
    "    return model\n",
    "\n",
    "\n",
    "# Train the model using Keras\n",
    "model = create_model()\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=32, verbose=2, validation_split=0.1)\n",
    "\n",
    "# Evaluate the model\n",
    "train_loss = model.evaluate(X_train, y_train)\n",
    "test_loss = model.evaluate(X_test, y_test)\n",
    "print(\"Train loss:\", train_loss)\n",
    "print(\"Test loss:\", test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d189e086-843c-4512-8b8b-8e81f339e345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate performance metrics\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"MAE:\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2897c5f-9ab6-4b35-b618-987c3e6e5987",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print(f\"RMSE: {rmse}\")\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(y_test, label=f'Actual values')\n",
    "plt.plot(y_pred, label='Predictions', linestyle='dashed')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Scaled Precipitation')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44a8f2c-65ba-4f8d-a827-99e6b4c44b3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 - Spark (local)",
   "language": "python",
   "name": "spark-3-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
